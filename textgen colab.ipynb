{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBe36HO_ZwOv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, GRU, Embedding\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afxP1qUCZ7Gi"
      },
      "outputs": [],
      "source": [
        "def text_cleaner(text):\n",
        "    # lower case text\n",
        "    newString = text.lower()\n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    # remove punctuations\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
        "    long_words=[]\n",
        "    # remove short word\n",
        "    for i in newString.split():\n",
        "        if len(i)>=3:\n",
        "            long_words.append(i)\n",
        "    return (\" \".join(long_words)).strip()\n",
        "\n",
        "# Open the file and read its contents\n",
        "with open('/content/lyrics_dataset.txt', 'r') as file:\n",
        "    data_text = file.read()\n",
        "\n",
        "# preprocess the text\n",
        "data_new = text_cleaner(data_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjMh_j1_Z_HB",
        "outputId": "8148d86d-40ba-4dff-8c0f-ea3812fec56a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 311172\n"
          ]
        }
      ],
      "source": [
        "def create_seq(text):\n",
        "    length = 30\n",
        "    sequences = list()\n",
        "    for i in range(length, len(text)):\n",
        "        # select sequence of tokens\n",
        "        seq = text[i-length:i+1]\n",
        "        # store\n",
        "        sequences.append(seq)\n",
        "    print('Total Sequences: %d' % len(sequences))\n",
        "    return sequences\n",
        "\n",
        "# create sequences\n",
        "sequences = create_seq(data_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ6etOtqZ_E5"
      },
      "outputs": [],
      "source": [
        "# create a character mapping index\n",
        "chars = sorted(list(set(data_new)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "def encode_seq(seq):\n",
        "    sequences = list()\n",
        "    for line in seq:\n",
        "        # integer encode line\n",
        "        encoded_seq = [mapping[char] for char in line]\n",
        "        # store\n",
        "        sequences.append(encoded_seq)\n",
        "    return sequences\n",
        "\n",
        "# encode the sequences\n",
        "sequences = encode_seq(sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imRBKbbyZ_Cx",
        "outputId": "a6d2d64e-2daf-4b2b-b06f-df77286216f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (280054, 30) Val shape: (31118, 30)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# vocabulary size\n",
        "vocab = len(mapping)\n",
        "sequences = np.array(sequences)\n",
        "# create X and y\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "# one hot encode y\n",
        "y = to_categorical(y, num_classes=vocab)\n",
        "# create train and validation sets\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcQ3w6T0Z_A5",
        "outputId": "2645b34f-e6f0-459d-c92f-032cba513a8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 30, 50)            1350      \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 150)               90900     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 27)                4077      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 96327 (376.28 KB)\n",
            "Trainable params: 96327 (376.28 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab, 50, input_length=30, trainable=True))\n",
        "model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\n",
        "model.add(Dense(vocab, activation='softmax'))\n",
        "print(model.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n"
      ],
      "metadata": {
        "id": "0ulgcorfzJhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model\n",
        "model.fit(X_tr, y_tr, epochs=10, verbose=2, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaHBp46AzLd4",
        "outputId": "40eb847c-da07-4a2c-ad45-7dfc10dfe635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "8752/8752 - 735s - loss: 1.5214 - acc: 0.5316 - val_loss: 1.4266 - val_acc: 0.5557 - 735s/epoch - 84ms/step\n",
            "Epoch 2/10\n",
            "8752/8752 - 724s - loss: 1.4292 - acc: 0.5561 - val_loss: 1.3840 - val_acc: 0.5695 - 724s/epoch - 83ms/step\n",
            "Epoch 3/10\n",
            "8752/8752 - 732s - loss: 1.3908 - acc: 0.5649 - val_loss: 1.3503 - val_acc: 0.5810 - 732s/epoch - 84ms/step\n",
            "Epoch 4/10\n",
            "8752/8752 - 724s - loss: 1.3685 - acc: 0.5708 - val_loss: 1.3355 - val_acc: 0.5849 - 724s/epoch - 83ms/step\n",
            "Epoch 5/10\n",
            "8752/8752 - 725s - loss: 1.3554 - acc: 0.5753 - val_loss: 1.3247 - val_acc: 0.5916 - 725s/epoch - 83ms/step\n",
            "Epoch 6/10\n",
            "8752/8752 - 725s - loss: 1.3458 - acc: 0.5770 - val_loss: 1.3161 - val_acc: 0.5927 - 725s/epoch - 83ms/step\n",
            "Epoch 7/10\n",
            "8752/8752 - 729s - loss: 1.3380 - acc: 0.5793 - val_loss: 1.3063 - val_acc: 0.5938 - 729s/epoch - 83ms/step\n",
            "Epoch 8/10\n",
            "8752/8752 - 751s - loss: 1.3335 - acc: 0.5809 - val_loss: 1.3060 - val_acc: 0.5940 - 751s/epoch - 86ms/step\n",
            "Epoch 9/10\n",
            "8752/8752 - 723s - loss: 1.3297 - acc: 0.5818 - val_loss: 1.3099 - val_acc: 0.5946 - 723s/epoch - 83ms/step\n",
            "Epoch 10/10\n",
            "8752/8752 - 728s - loss: 1.3261 - acc: 0.5828 - val_loss: 1.2991 - val_acc: 0.5986 - 728s/epoch - 83ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7bf4d49186a0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_GOvKKIZ--p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "    in_text = seed_text\n",
        "    # generate a fixed number of characters\n",
        "    for _ in range(n_chars):\n",
        "        # encode the characters as integers\n",
        "        encoded = [mapping[char] for char in in_text]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # predict character\n",
        "        yhat = model.predict(encoded, verbose=0)\n",
        "        # map predicted integer to character\n",
        "        out_char = ''\n",
        "        for char, index in mapping.items():\n",
        "            if index == np.argmax(yhat):\n",
        "                out_char = char\n",
        "                break\n",
        "        # append to input\n",
        "        in_text += out_char\n",
        "    return in_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMmvzqUOZ-8i",
        "outputId": "2270e81e-8ad8-46aa-f0fd-6c27c245fec8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            " river and the s\n"
          ]
        }
      ],
      "source": [
        "inp = \" \"\n",
        "print(len(inp))\n",
        "print(generate_seq(model,mapping,30,inp.lower(),15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3oMPspu1Y4s",
        "outputId": "422c6eaa-83bf-4ea8-acf3-407023c7ae2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 3.6661532688250107\n",
            "Generated Text 1: hellow the same say what you need say when you and the same say what you need say when you and the same s\n",
            "Generated Text 2: the catch the same say what you need say when you and the same say what you need say when you and the same \n",
            "Generated Text 3: machine learning the same say what you need say when you and the same say what you need say when you and the same sa\n"
          ]
        }
      ],
      "source": [
        "from math import exp\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Calculate Perplexity\n",
        "def calculate_perplexity(model, X_val, y_val):\n",
        "    # Evaluate the model on validation set\n",
        "    loss = model.evaluate(X_val, y_val, verbose=0)\n",
        "    cross_entropy = loss[0]\n",
        "    # Calculate perplexity\n",
        "    perplexity = exp(cross_entropy)\n",
        "    return perplexity\n",
        "\n",
        "# Generate Text Samples\n",
        "def generate_text_samples(model, mapping, seq_length, seed_texts, n_chars):\n",
        "    generated_texts = []\n",
        "    for seed_text in seed_texts:\n",
        "        generated_text = generate_seq(model, mapping, seq_length, seed_text.lower(), n_chars)\n",
        "        generated_texts.append(generated_text)\n",
        "    return generated_texts\n",
        "\n",
        "# Calculate perplexity\n",
        "perplexity = calculate_perplexity(model, X_val, y_val)\n",
        "print(\"Perplexity:\", perplexity)\n",
        "\n",
        "# Generate text samples\n",
        "seed_texts = [\"hello\", \"the cat\", \"machine learning\"]\n",
        "generated_texts = generate_text_samples(model, mapping, 30, seed_texts, 100)\n",
        "for i, text in enumerate(generated_texts):\n",
        "    print(f\"Generated Text {i+1}: {text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lguqx_hvZ-17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97bbf002-3f28-4cb6-d736-89360e7134a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save('text quality.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiOjEJGEZ-y6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}